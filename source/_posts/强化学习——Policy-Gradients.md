---
title: 强化学习——Policy Gradients
catalog: true
date: 2018-05-25 11:47:31
subtitle: 
header-img: nn.jpg
tags:
- 机器学习
---
## 一、与其他强化学习方法不同
强化学习是一个通过奖惩来学习正确行为的机制。

家族中有很多种不一样的成员，有学习奖惩值，根据自己认为的高价值选行为，比如 {% post_link 强化学习——Q-learning Q-learning %}，{% post_link 强化学习——Deep-Q-Network Deep-Q-Network %}，也有不通过分析奖励值，直接输出行为的方法，这就是 Policy Gradients 了。

甚至我们可以为 Policy Gradients 加上一个神经网络来输出预测的动作。对比起以值为基础的方法，Policy Gradients 直接输出动作的最大好处就是，它能在一个连续区间内挑选动作。

而基于值的，比如 Q-learning，它如果在无穷多的动作中计算价值，从而选择行为，这可吃不消。

## 二、误差传递与更新
有了神经网络当然方便，但是，我们怎么进行神经网络的误差反向传递呢？

Policy Gradients 的误差又是什么呢？

答案是**没有误差**！

但是他的确是在进行某一种的反向传递。这种反向传递的目的是让这次被选中的行为更有可能在下次发生。

但是我们要怎么确定这个行为是不是应当被增加被选的概率呢？这时候 Reward 奖惩正可以在这时候派上用场。

## 三、具体更新步骤
输入观测的信息通过神经网络分析，解设选出了 A 行为，我们直接进行反向传递，使之下次被选的可能性增加，但是奖惩信息却告诉神经网络，这次的行为是不好的，那我们的动作可能性增加的幅度随之被减低，这样就能靠奖励来左右我们的神经网络反向传递。

再来举个例子，假如这次的观测信息让神经网络选择了 B 行为，B 行为随之想要进行反向传递，使 B 的行为下次被多选一点，这时奖惩信息告诉神经网络这是好行为，那我们就在这次反向传递的时候加大力度，让它下次被多选的幅度更猛烈！这就是 Policy Gradients 的核心思想了。


*以上内容参考[莫凡Python](https://morvanzhou.github.io/tutorials/machine-learning/)*。
