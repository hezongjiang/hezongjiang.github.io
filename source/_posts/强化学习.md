---
title: 强化学习
catalog: true
date: 2018-03-22 13:59:23
subtitle:
header-img: timg.jpeg
tags:
- 机器学习
---
## 一、什么是强化学习
强化学习是一类算法，是让计算机实现从一开始完全随机的进行操作，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程。让计算机在不断的尝试中更新自己的行为，从而一步步学习如何操自己的行为得到高分。

它主要包含四个元素，**Agent**、**环境状态**、**行动**、**奖励**，强化学习的目标就是获得最多的累计奖励。

让我们想象一下比赛现成：

计算机有一位虚拟的裁判，这个裁判他不会告诉你如何行动，如何做决定，他为你做的事只有给你的行为打分，最开始，计算机完全不知道该怎么做，行为完全是随机的，那计算机应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为.

计算机就是 Agent，他试图通过采取行动来操纵环境，并且从一个状态转变到另一个状态，当他完成任务时给高分(奖励)，但是当他没完成任务时，给低分(无奖励)。这也是强化学习的核心思想。所以强化学习具有分数导向性。

我们换一个角度来思考.这种分数导向性好比我们在监督学习中的正确标签。

![](https://upload-images.jianshu.io/upload_images/2708793-0d7139e90776f94f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 二、强化学习方法
**1、Model-free 和 Model-based**

如果不理解环境，环境给了什么就是什么，我们就把这种方法叫做 Model-free，这里的 Model 就是用模型来表示环境，理解环境就是学会了用一个模型来代表环境，所以这种就是 Model-based 方法。

Model-free 的方法有很多，像 Q learning、Sarsa、Policy Gradients 都是从环境中得到反馈然后从中学习。而 Model-based RL 只是多了一道程序，为真实世界建模，也可以说他们都是 Model-free 的强化学习，只是 Model-based 多出了一个虚拟环境，我们可以先在虚拟环境中尝试，如果没问题，再拿到现实环境中来。最终 Model-based 还有一个杀手锏，是 Model-free 所不具备的，那就是想象力.

Model-free 中，计算机只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 Model-based，能通过想象来预判断接下来将要发生的所有情况，然后选择这些想象情况中最好的那种，并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。

**2、基于概率 和 基于价值**

基于概率是强化学习中最直接的一种，他通所处的环境，输出下一步要采取的各种动作的概率，根据概率采取行动，所以每种动作都有可能被选中，只是概率不同。而基于价值的方法输出则是所有动作的价值，根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为肯定，就选价值最高的，而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他.

但是对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个概率分布在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢?

**3、回合更新 和 单步更新**

强化学习还能用另外一种方式分类，回合更新和单步更新。假设强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，需要等待游戏结束再总结这一回合，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束，这样边玩边学习。

**4、在线学习 和 离线学习**

所谓在线学习，就是指必须本人在场，并且一定是本人一边行动边一学习。而离线学习是你可以选择自己行动，也可以选择看着别人行动，通过看别人行动来学习别人的行为准则，离线学习 同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。

## 三、强化学习算法
强化学习是一个大家族，包含了很多种算法，接下来也会提到之中一些比较知名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的 Q_learning、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习 等等 。

## 四、Q_learning
假设我们的行为准则已经学习好了，现在我们处于状态 s(tate) 1，有两个行为 a(ction) 1、a(ction) 2，在这种 s1 状态下，a2 带来的潜在奖励要比 a1 高，这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替，在Q表格中，Q(s1，a1) = -2，要小于 Q(s1，a2) = 1，所以我们判断要选择 a2 作为下一个行为。

现在我们的状态更新成 s2，我们还是有两个同样的选择，重复上面的过程，在行为准则 Q 表中寻找 Q(s2，a1)、Q(s2，a2) 的值，并比较他们的大小，选取较大的一个。接着根据 a2 我们到达 s3 并在此重复上面的决策过程。

Q_learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改、提升的。

|       Q   |   a1  | a2 |
| :--------: | :----------: |:---------:|
| s1 | -2  | 1  |
| s2 | -4  |   2   |

根据 Q 表的估计，因为在 s1 中，a2 的值比较大，通过之前的决策方法，我们在 s1 采取了 a2，并到达 s2，这时我们开始更新用于决策的 Q 表，接着我们分别看看两种行为哪一个的 Q 值大。

比如说 Q(s2，a2) 的值比 Q(s2，a1) 的大，所以我们把大的值乘上一个衰减值 gamma (比如是0.9) 并加上到达 s2 时所获取的奖励 R(eward)，因为会获取实实在在的奖励 R，我们将这个作为我现实中 Q(s1，a2) 的值，但是我们之前是根据 Q 表估计 Q(s1，a2) 的值。所以有了现实和估计值，我们就能更新Q(s1，a2)，根据估计与现实的差距，将这个差距乘以一个学习效率 alpha 累加上旧的 Q(s1，a2) 的值，变成新的值。

![Q 表更新方式](https://upload-images.jianshu.io/upload_images/2708793-055034e4da8d984e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但时刻记住，我们虽然用 maxQ(s2) 估算了一下 s2 状态，但还没有在 s2 做出任何的行为，s2 的行为决策要等到更新完了以后再重新另外做。这就是 Off-Policy 的 Q-learning 是如何决策和学习优化决策的过程

```
Initialize Q arbitrarily // 随机初始化Q表
Repeat (for each episode): // 每一次游戏，从开始到结束是一个episode
Initialize S // S为初始位置的状态
Repeat (for each step of episode):
Choose a from s using policy derived from Q(ε-greedy) //根据当前Q和位置S，使用一种策略，得到动作A，这个策略可以是ε-greedy等
Take action a, observe r // 做了动作A，到达新的位置S'，并获得奖励R，奖励可以是1，50或者-1000
Q(S,A) ← Q(S,A) + α*[R + γ*maxQ(S',a))-Q(s,a)] //在Q中更新S
S ← S'
until S is terminal //即到游戏结束为止
```
Q-learning的详细介绍可以看看这篇文章{[强化学习——Q-learning](https://www.jianshu.com/p/44ce8a55d820)}。

## 五、Sarsa
Sarsa 的决策部分和 Q learning 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩. 但是不同的地方在于 Sarsa 的更新方式是不一样的。

同样，我们会经历状态 s1，然后再挑选一个带来最大潜在奖励的动作 a2，这样我们就到达了状态 s2，而在这一步，如果你用的是 Q learning，你会观看一下在 s2 上选取哪一个动作会带来最大的奖励，但是在真正要做决定时，却不一定会选取到那个带来最大奖励的动作，Q-learning 在这一步只是估计了一下接下来的动作值. 而 Sarsa 到做到，在 s2 这一步估算的动作也是接下来要做的动作. 所以 Q(s1，a2) 现实的计算值，我们也会稍稍改动，去掉maxQ，取而代之的是在 s2 上我们实实在在选取的 a2 的 Q 值. 最后像 Q learning 一样，求出现实和估计的差距 并更新 Q 表里的 Q(s1，a2)。
```
Initialize Q arbitrarily // 随机初始化Q表
Repeat (for each episode): // 每一次游戏，从开始到结束是一个episode
Initialize S // S为初始位置的状态
Choose a from s using policy derived from Q(ε-greedy) 
Repeat (for each step of episode):
Take action a, observe r, s'
Choose a' from s' using policy derived from Q(ε-greedy) 
Q(S,A) ← Q(S,A) + α*[R + γ*Q(S',a')-Q(s,a)] //在Q中更新S
S ← S'; a← a'
until S is terminal //即到游戏结束为止
```
Sarsa的详细介绍可以戳这里[强化学习——Sarsa](https://www.jianshu.com/p/a2a2efcee79c)。

## 六、DQN
看到这里，不知道你有没有发现一个问题。

我们使用表格来存储每一个状态 state 和在这个 state 每个行为 action 所拥有的 Q 值。而当今问题是在太复杂，状态比天上的星星还多（比如下围棋）。如果全用表格来存储它们，恐怕计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中，有一种方法对这种事情很在行，那就是神经网络。

我们可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式的是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q-learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。

我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。

如果你想进一步了解DQN，可以看看这篇文章[强化学习——Deep Q Network](https://www.jianshu.com/p/a95d7f130086)。

## 七、Policy Gradients
观测的信息通过神经网络分析，选出了一个的行为，我们直接进行反向传递，使之下次被选的可能性增加，但是奖惩信息却告诉我们，这次的行为是不好的，那我们的动作可能性增加的幅度随之被减低，这样就能靠奖励来左右我们的神经网络反向传递。

假如这次的观测信息让神经网络选择了另一个行为，右边的行为随之想要进行反向传递，使右边的行为下次被多选一点，这时，奖惩信息也来了，告诉我们这是好行为，那我们就在这次反向传递的时候加大力度，让它下次被多选的幅度更猛烈！这就是 Policy Gradients 的核心思想了。

详细请看[强化学习——Policy Gradients](https://www.jianshu.com/p/ba7eeeca5da0)。

## 八、Actor Critic
有了像 Q-learning这么伟大的算法，为什么还要瞎折腾出一个 Actor-Critic？原来 Actor-Critic 的 Actor 的前生是 Policy Gradients，这能让它毫不费力地在连续动作中选取合适的动作，而 Q-learning 做这件事会瘫痪。那为什么不直接用 Policy Gradients 呢？因为 Actor Critic 中的 Critic 的前生是 Q-learning 或者其他的以值为基础的学习法，能进行单步更新，而传统的 Policy Gradients 则是回合更新，这降低了学习效率。

现在我们有两套不同的体系，Actor 和 Critic，他们都能用不同的神经网络来代替。现实中的奖惩会左右 Actor 的更新情况。Policy Gradients 也是靠着这个来获取适宜的更新。那么何时会有奖惩这种信息能不能被学习呢？这看起来不就是以值为基础的强化学习方法做过的事吗？

那我们就拿一个 Critic 去学习这些奖惩机制，学习完了以后，由 Actor 来指手画脚，由 Critic 来告诉 Actor 你的那些指手画脚哪些指得好，哪些指得差，Critic 通过学习环境和奖励之间的关系，能看到现在所处状态的潜在奖励，所以用它来指点 Actor 便能使 Actor 每一步都在更新，如果使用单纯的 Policy Gradients，Actor 只能等到回合结束才能开始更新。

*以上内容参考[莫凡Python](https://morvanzhou.github.io/tutorials/machine-learning/)*。