---
title: L1 / L2 正规化 (Regularization)
catalog: true
date: 2018-03-08 20:54:24
subtitle:
header-img: timg.jpeg
tags:
- 机器学习
---
## 一、过拟合

![](http://upload-images.jianshu.io/upload_images/2708793-f4762fefd105016c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们知道，过拟合就是所谓的模型对可见的数据过度自信，非常完美的拟合上了这些数据，如果具备过拟合的能力，那么这个方程就可能是一个比较复杂的非线性方程 ，正是因为这里的 x^2 和 x^3 使得这条虚线能够被弯来弯去，所以整个模型就会特别努力地去学习作用在 x^3 和 x^2 上的 c d 参数。但是我们期望模型要学到的却是这条蓝色的曲线。因为它能更有效地概括数据。而且只需要一个 y=a+bx 就能表达出数据的规律。或者是说，蓝色的线最开始时，和红色线同样也有 c d 两个参数，可是最终学出来时，c 和 d 都学成了0，虽然蓝色方程的误差要比红色大，但是概括起数据来还是蓝色好。那我们如何保证能学出来这样的参数呢? 这就是 l1 l2 正规化出现的原因。

## 二、L1 / L2 正规化 (Regularization)

![](http://upload-images.jianshu.io/upload_images/2708793-059bc9b7274faebd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 对于刚刚的线条，我们一般用这个方程来求得模型 y(x) 和真实数据 y 的误差，而 L1、L2 就只是在这个误差公式后面多加了一个东西，让误差不仅仅取决于拟合数据拟合的好坏，而且取决于像刚刚 c d 那些参数的值的大小。如果是每个参数的平方，那么我们称它为 L2正规化，如果是每个参数的绝对值，我们称为 L1 正规化。

## 三、正规化核心思想
我们拿 L2正规化来探讨一下，机器学习的过程是一个通过修改参数来减小误差的过程，可是在减小误差的时候非线性越强的参数，比如在 x^3 旁边的 参数 就会被修改得越多，因为如果使用非线性强的参数就能使方程更加曲折，也就能更好的拟合上那些分布的数据点。可是它这种态度招到了误差方程的强烈反击，误差方程就说: no no no no，我们是一个团队，虽然你厉害，但也不能仅仅靠你一个人，万一你错了，我们整个团队的效率就突然降低了，这就是整套正规化算法的核心思想。那 L1，L2 正规化又有什么不同呢?

## 四、图像化

![](http://upload-images.jianshu.io/upload_images/2708793-6b30d380b7f108b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

想象现在只有两个参数 theta1 theta2 要学，蓝色的圆心是误差最小的地方，而每条蓝线上的误差都是一样的。正规化的方程是在黄线上产生的额外误差，在黄圈上的额外误差也是一样。所以在蓝线和黄线 交点上的点能让两个误差的合最小。这就是 theta1 和 theta2 正规化后的解。要提到另外一点是，使用 L1 的方法，我们很可能得到的结果是只有 theta1 的特征被保留，所以很多人也用 L1 正规化来挑选对结果贡献最大的重要特征。但是 L1 的结并不是稳定的。比如用批数据训练，每次批数据都会有稍稍不同的误差曲线,

![](http://upload-images.jianshu.io/upload_images/2708793-31b809e9c7560d46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

L2 针对于这种变动，白点的移动不会太大，而 L1的白点则可能跳到许多不同的地方 ，因为这些地方的总误差都是差不多的。侧面说明了 L1 解的不稳定性。
